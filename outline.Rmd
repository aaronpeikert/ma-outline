---
title: "Multilevel Machine Learning"
subtitle: "Random Effekts for Black Box Models"
author: "Aaron Peikert"
date: "`r Sys.Date()`"
bibliography: MA.bib
output: pdf_document
linestretch: 1.5
lang: de-DE
fontsize: 11
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require("pacman"))install.packages("pacman")
pacman::p_load("tidyverse", "here", "pander")
```

# Research Question

Can multimodel inference exploit multilevel structures to increase predictive performance?

# Outline

The usual target of statistical modelling in psychology is new samples, but such an approach does severely limit what psychologists can attempt to study. Stemming from @cattellDataBox1988's idea of the "data box", several authors [e.g. @nesselroadeBehavioralScienceMeasurement2016; @revelleDataBoxSubject2016; @revelleModelPersonalityThree2015] argue that the individual should be the main focus of the analysis. That means generalising not to new individuals, but to the behaviour of the same individuals at a later time point. The attempt of using statistical models, not explicitly designed for such endeavour, assumes that conditional on the measured features all individuals are interchangeable [citation needed]. The most common justification of interchangeability is a variation of Laplace's principle of indifference. As long as no evidence suggests otherwise, one has no choice but to assume interchangeability [citation needed]. One may argue that researchers in the field of psychology are very rarely in such state of oblivion that would justify complete interchangeability of individuals.

However, interchangeability on some level is a precondition of any predictive model [citation needed]. Finding a more defendable --- but still practical --- assumption of exchangeability for individualised prediction might prove to be a worthwhile objective. Indeed researcher found a particularly elegant approach. The assumption that each individual has its parametrised version of a joint model structure, with the parameters drawn from a joint distribution, leads to a direct extension of the linear model [citation needed]. Such parameters are called random effects and the resulting model mixed or multilevel models. Multilevel models require several decisions from the researcher, most considerable: which variables to include, which parameters to model as random and which functional form (that is linear in its parameters) to assume [citation needed]. Ideally, intimate knowledge of the subject matter informs these decisions. This rigour allows unbiased estimates and formal inference and is therefore of the highest importance to research. Unfortunately, this approach does not extend well to situations where the researcher gained enough knowledge to refuse exchangeability but has limited knowledge about the functional form of the relationship. Such a situation is reoccurring in some methodological approaches, most notably the experience sampling method and neuroimaging methods. These examples share three properties that contribute to that. First, repeated measurement of the same individual is conducted, where the order of the measurement is of secondary concern. Second, a multitude of variables may or may not influence the outcome. Third, individuals are, to some extent, idiosyncratic. These properties resemble the dimensions of a data box.

Occasions
: Conditional on the individual and the variables measured, the occasions are interchangeable.

Variables
: The number of variables under consideration impede the construction of a strict functional form.

Persons
: Persons may or may not share properties but are generally not assumed to be interchangeable.

These three properties describe the broader class of data boxes under consideration in this thesis. Exchangeability of occasions is assumed, not because it is a peculiarly realistic assumption, but because it is a convenient one. However, the averaging in experience sampling methods over multiple similar occasions or in neuroimaging over trials makes this assumption routinely. The researchers' ignorance again justifies this; if there is no evidence to suspect the opposite, there is no alternative. Neither the convenience nor the ignorance argument should be an excuse to fail to check this assumption. The second assumption that a large number of variables in a model makes it harder to specify a strict model is probably a truism that does not need more justification. The third assumption, whether or not there is idiosyncrasy in human behaviour is a debate probably as old as the field of psychology. For the purpose here, it is sufficient to say that there might be a set of variables that is unobserved, where at least one variable varies with the person and relate to the outcome.

The approach here considered builds upon the belief that good modelling means to find the best approximation of reality, given the data. Modelling is understood as an optimisation problem, not a decision problem. The crucial question is therefore not "Is this model true?" but "Is this model a good approximation?" [@boxRobustnessStrategyScientific1979]. If one refuses the assumption that individuals are interchangeable, one does not necessarily need to refuse a model that makes this assumption. Likewise, just because a model is correctly specified--- even in the literal sense --- it might not necessarily be the best model.

The question of exchangeability is posed as a model evaluation problem with two competing models. One model fitted on the population under the assumption of exchangeability of individuals, the other fitted on the individual assuming exchangeability of situations. When faced with two competing models, researchers are usually inclined to perform some model selection procedure. However, from an information-theoretic standpoint not selecting but weighting model predictions may be more appropriate [@akaikeBayesianExtensionMinimum1979; @bozdoganModelSelectionAkaike1987; @burnhamModelSelectionMultimodel2002, p. 149-150]. While not justified as rigorously, the combination of multiple models is also a popular method in machine learning [@opitzPopularEnsembleMethods1999].

Treating exchangeability as a model evaluation problem makes it possible to conceptualise degrees of exchangeability as the populations model fit conditional on the set of models (population and individual model). Or reformulated, how well does the population model fit compared with the individual model? Such formulation reduces the problem too finding an adequate estimator for model fit and an adequate transformation to condition it on the model set. Luckily there are well-developed model evaluation techniques, namely information criteria (e.g. Cp, AIC, BIC, WAIC, SIC, MDL) and resampling techniques (e.g. cross-validation, bootstrapping, independent validation). These measures of model fit have to be scaled to express a model fit conditional on the set of models under consideration. For the information criteria, such transformation is readily available (e.g. Akaike Weights or Schwarz Weights) or are easily follow (e.g. for WAIC & SIC). Both information criteria and resampling methods set out to estimate the value of a loss function on a hypothetical holdout set conditional on the model but information criteria generally yield a linear transformation of that loss function. Resampling methods are more direct in this regard, and therefore the conditional fit is just their relative share of the sum of loss.

# Sessioninfo

```{r}
sessioninfo::platform_info() %>%
  unclass() %>% 
  pander()
```

# References
