---
title: "Multilevel Machine Learning"
subtitle: "Random Effekts for Black Box Models"
author: "Aaron Peikert"
date: "`r Sys.Date()`"
bibliography: MA.bib
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require("pacman"))install.packages("pacman")
pacman::p_load("tidyverse", "here", "pander")
```

The usual target of statistical modeling in psychology is new samples, but such an approach does severely limit what psychologists can attempt to study. Stemming from @cattellDataBox1988 idea of the "data box", several authors [e.g. @nesselroadeBehavioralScienceMeasurement2016; @revelleDataBoxSubject2016; @revelleModelPersonalityThree2015] argue that the individual should be the main focus of the analysis. That means generalizing not to new individuals, but to the behavior of the same individuals at a later time point. The attempt of using statistical models, that are not specifically designed for such endeavor, assumes that conditional on the measured features all individuals are interchangeable. This assumption can rarely be justified in psychological research.

While there is much research devoted to mixed effect models resulting in elegant solutions to problems such as the above, it is primarily focused on providing unbiased estimates instead of predictive accuracy and requires a lot of thought from the researcher. It might be a fruitful approach to fit arbitrary predictive models simultaneously on the individual and the population. When faced with two competing models researchers are usually inclined to perform some sort of model selection procedure, however from an information-theoretic standpoint not selecting but weighting model predictions may be more appropriate [@akaikeBayesianExtensionMinimum1979; @bozdoganModelSelectionAkaike1987; @burnhamModelSelectionMultimodel2002, p. 149-150]. The proposal is, therefore, to use multimodel inference to exploit multilevel structures within the data to increase predictive performance. In a first step, it is evaluated whether or not this thinking is justified and therefore existing solutions based on maximum likelihood models (e.g. Akaike‑Weights) may be used. In a second step, the link between information criteria and cross-validation procedures is used to generalize this method to arbitrary predictive models.

One of the simplest problems of the type discussed: Imagine a teacher is rated by his students and we set out to predict the student's ratings of the next lesson with the past lessons rating. Two approaches come to mind, either we predict each student's rating by his past rating or we predict each student's rating by the overall average of the students. One assumes that there is nothing to learn from the other student's ratings, the other that there is nothing to learn from the individual beyond representing a sample of the population. Each assumption represents historically important lines of psychological research, namely experimental and differential psychology. If we had a large sample of each student's ratings, regardless of which assumption is true, predicting the individual rating by past ratings of the same individual would perform always better or equally good. So why is it that we often choose to model the individual by some sort of average of the population? Because a lot of statistical precision can be gained, by making this simplifying assumption, since there is much more information about the population than about the individual. This tradeoff is often called the „bias-variance“ tradeoff. While one method of inference makes fewer assumptions, therefore, less bias, it results in bigger variance due to the smaller sample.

# Research Question

**Can the weighting of multiple models adress multilevel structure in data?**

## Intermediate Questions

1. Are criteria stemming from information-theory up for the task under optimal conditions?
2. How do perform resampling/cross-validation strategies under the same conditions concerning precision and unbiasedness?
3. How do perform information-criteria vs. cross-validation under suboptimal conditions (not meeting assumptions of the information criteria)?
4. What information does the mixing parameter carry? Is the inconsistency of the AIC a problem?
5. Is the model selection uncertainty different for the two models? Does it influence the results? Can it be fixed?

# Sessioninfo

```{r}
sessioninfo::platform_info() %>%
  unclass() %>% 
  pander()
```

# References
